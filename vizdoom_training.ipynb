{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba53d40",
   "metadata": {},
   "source": [
    "# Training World Models in VizDoom from Scratch\n",
    "\n",
    "This notebook documents the process of setting up and training a World Models implementation for VizDoom with specific dependencies:\n",
    "\n",
    "- Ubuntu 16.04\n",
    "- Python 3.5.4\n",
    "- TensorFlow 1.8.0\n",
    "- NumPy 1.13.3\n",
    "- VizDoom Gym Levels (Latest commit 60ff576 on Mar 18, 2017)\n",
    "- OpenAI Gym 0.9.4\n",
    "- cma 2.2.0\n",
    "- mpi4py 2\n",
    "- Jupyter Notebook\n",
    "\n",
    "The World Models approach consists of three components:\n",
    "1. **VAE (Vision)**: Compresses the high-dimensional visual input into a latent representation\n",
    "2. **MDN-RNN (Memory)**: Predicts future latent states based on current states and actions\n",
    "3. **Controller**: Maps latent states to actions using a simple neural network\n",
    "\n",
    "Let's start by setting up our Docker environment to ensure we have the correct dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7146f5",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Dependencies\n",
    "\n",
    "First, we'll create a Dockerfile that sets up the correct environment for our experiment. This Dockerfile will:\n",
    "1. Use Ubuntu 16.04 as the base image\n",
    "2. Install Python 3.5.4\n",
    "3. Install TensorFlow 1.8.0, NumPy 1.13.3, and other dependencies\n",
    "4. Install VizDoom and the required gym environments\n",
    "\n",
    "Let's create this Dockerfile and build our Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7bcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.vizdoom\n",
    "\n",
    "FROM ubuntu:16.04\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    cmake \\\n",
    "    git \\\n",
    "    libboost-all-dev \\\n",
    "    libgtk2.0-dev \\\n",
    "    libsdl2-dev \\\n",
    "    python3-dev \\\n",
    "    python3-pip \\\n",
    "    python3-numpy \\\n",
    "    wget \\\n",
    "    zlib1g-dev \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Python 3.5.4\n",
    "RUN apt-get update && apt-get install -y python3.5-dev python3.5-tk\n",
    "\n",
    "# Upgrade pip\n",
    "RUN pip3 install --upgrade pip\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip3 install \\\n",
    "    tensorflow==1.8.0 \\\n",
    "    numpy==1.13.3 \\\n",
    "    gym==0.9.4 \\\n",
    "    cma==2.2.0 \\\n",
    "    mpi4py==2.0.0 \\\n",
    "    matplotlib==2.2.3 \\\n",
    "    jupyter \\\n",
    "    pillow \\\n",
    "    scipy==1.0.0\n",
    "\n",
    "# Install VizDoom\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    libbz2-dev \\\n",
    "    libffi-dev \\\n",
    "    libfreetype6-dev \\\n",
    "    libjpeg-dev \\\n",
    "    liblzma-dev \\\n",
    "    libncurses5-dev \\\n",
    "    libncursesw5-dev \\\n",
    "    libpng-dev \\\n",
    "    libreadline-dev \\\n",
    "    libssl-dev \\\n",
    "    libsqlite3-dev \\\n",
    "    libx11-dev \\\n",
    "    libgl1-mesa-dev \\\n",
    "    tk-dev\n",
    "\n",
    "# Clone and build VizDoom\n",
    "RUN git clone https://github.com/mwydmuch/ViZDoom.git \\\n",
    "    && cd ViZDoom \\\n",
    "    && python3 setup.py build \\\n",
    "    && python3 setup.py install\n",
    "\n",
    "# Clone VizDoom gym environments\n",
    "RUN git clone https://github.com/ppaquette/gym-doom.git \\\n",
    "    && cd gym-doom \\\n",
    "    && git reset --hard 60ff576 \\\n",
    "    && pip3 install -e .\n",
    "\n",
    "# Create working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the code\n",
    "COPY doomrnn/ /app/doomrnn/\n",
    "\n",
    "# Expose port for Jupyter Notebook\n",
    "EXPOSE 8888\n",
    "\n",
    "# Default command\n",
    "CMD [\"jupyter\", \"notebook\", \"--ip=0.0.0.0\", \"--port=8888\", \"--no-browser\", \"--allow-root\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0995d",
   "metadata": {},
   "source": [
    "Now, let's build the Docker image. This may take some time as it's installing all the dependencies.\n",
    "\n",
    "```bash\n",
    "docker build -t worldmodels-vizdoom -f Dockerfile.vizdoom .\n",
    "```\n",
    "\n",
    "Once the image is built, we can run a container with the following command:\n",
    "\n",
    "```bash\n",
    "docker run -p 8888:8888 -v $(pwd):/app worldmodels-vizdoom\n",
    "```\n",
    "\n",
    "This will start a Jupyter Notebook server that we can access through our browser.\n",
    "\n",
    "Now, let's proceed with understanding and implementing the World Models architecture for VizDoom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffe4b8",
   "metadata": {},
   "source": [
    "## 2. Understanding the VizDoom Environment\n",
    "\n",
    "VizDoom provides a 3D environment based on the Doom game, allowing an agent to learn to navigate and act in a 3D world. The observations are RGB images from the agent's perspective, and the actions are discrete (move forward, turn left, turn right, shoot, etc.).\n",
    "\n",
    "The World Models paper uses the \"Take Cover\" scenario, where the agent needs to avoid fireballs thrown by enemies. Let's first explore this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fefba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code would run in the Docker container\n",
    "# Here we're showing what we would execute\n",
    "\n",
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('ppaquette/DoomTakeCover-v0')\n",
    "\n",
    "# Reset the environment\n",
    "observation = env.reset()\n",
    "\n",
    "# Render the environment\n",
    "plt.figure(figsize=(8, 6))\n",
    "imshow(observation)\n",
    "plt.title('VizDoom Take Cover - Initial Observation')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Get the observation space and action space\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8846b34",
   "metadata": {},
   "source": [
    "## 3. Overview of World Models Architecture\n",
    "\n",
    "The World Models architecture for VizDoom consists of three main components:\n",
    "\n",
    "1. **VAE (Vision)**: A variational autoencoder that compresses the high-dimensional visual input (RGB images) into a low-dimensional latent representation.\n",
    "2. **MDN-RNN (Memory)**: A recurrent neural network with a mixture density network output layer that predicts future latent states based on current states and actions.\n",
    "3. **Controller**: A simple neural network that maps latent states to actions.\n",
    "\n",
    "Let's implement each of these components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e909842",
   "metadata": {},
   "source": [
    "## 4. Implementing the VAE (Vision)\n",
    "\n",
    "The VAE takes the high-dimensional visual input (RGB images) and compresses it into a low-dimensional latent representation. This makes it easier for the agent to learn.\n",
    "\n",
    "The VAE architecture from the World Models paper consists of:\n",
    "- Encoder: Several convolutional layers that reduce the image to a low-dimensional latent vector\n",
    "- Latent space: A probabilistic representation of the input\n",
    "- Decoder: Several deconvolutional layers that reconstruct the image from the latent vector\n",
    "\n",
    "Let's implement the VAE for VizDoom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the VAE implementation for VizDoom\n",
    "# In the Docker container, this would be in doomrnn/vae.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "class ConvVAE(object):\n",
    "    def __init__(self, z_size=64, batch_size=100, learning_rate=0.0001, kl_tolerance=0.5, is_training=True, reuse=False, gpu_mode=True):\n",
    "        self.z_size = z_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_training = is_training\n",
    "        self.kl_tolerance = kl_tolerance\n",
    "        self.reuse = reuse\n",
    "        self.gpu_mode = gpu_mode\n",
    "        with tf.variable_scope('conv_vae', reuse=self.reuse):\n",
    "            if not gpu_mode:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    tf.logging.info('Model using cpu.')\n",
    "                    self._build_graph()\n",
    "            else:\n",
    "                tf.logging.info('Model using gpu.')\n",
    "                self._build_graph()\n",
    "            self._init_session()\n",
    "            self._init_saver()\n",
    "            \n",
    "    def _build_graph(self):\n",
    "        # Placeholders for input and output\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n",
    "        \n",
    "        # Encoder\n",
    "        h = tf.layers.conv2d(self.x, 32, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv1\")\n",
    "        h = tf.layers.conv2d(h, 64, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv2\")\n",
    "        h = tf.layers.conv2d(h, 128, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv3\")\n",
    "        h = tf.layers.conv2d(h, 256, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv4\")\n",
    "        h = tf.reshape(h, [-1, 2*2*256])\n",
    "        \n",
    "        # VAE latent layers\n",
    "        self.mu = tf.layers.dense(h, self.z_size, name=\"enc_fc_mu\")\n",
    "        self.logvar = tf.layers.dense(h, self.z_size, name=\"enc_fc_log_var\")\n",
    "        self.sigma = tf.exp(self.logvar / 2.0)\n",
    "        self.epsilon = tf.random_normal([self.batch_size, self.z_size])\n",
    "        self.z = self.mu + self.sigma * self.epsilon\n",
    "        \n",
    "        # Decoder\n",
    "        h = tf.layers.dense(self.z, 4*256, name=\"dec_fc\")\n",
    "        h = tf.reshape(h, [-1, 1, 1, 4*256])\n",
    "        h = tf.layers.conv2d_transpose(h, 128, 5, strides=2, activation=tf.nn.relu, name=\"dec_deconv1\")\n",
    "        h = tf.layers.conv2d_transpose(h, 64, 5, strides=2, activation=tf.nn.relu, name=\"dec_deconv2\")\n",
    "        h = tf.layers.conv2d_transpose(h, 32, 6, strides=2, activation=tf.nn.relu, name=\"dec_deconv3\")\n",
    "        self.y = tf.layers.conv2d_transpose(h, 3, 6, strides=2, activation=tf.nn.sigmoid, name=\"dec_deconv4\")\n",
    "        \n",
    "        # Loss\n",
    "        # Reconstruction loss (binary cross entropy)\n",
    "        self.reconstruction_loss = tf.reduce_sum(\n",
    "            tf.square(self.x - self.y),\n",
    "            reduction_indices=[1, 2, 3]\n",
    "        )\n",
    "        self.reconstruction_loss = tf.reduce_mean(self.reconstruction_loss)\n",
    "        \n",
    "        # KL divergence loss\n",
    "        self.kl_loss = -0.5 * tf.reduce_sum(\n",
    "            (1 + self.logvar - tf.square(self.mu) - tf.exp(self.logvar)),\n",
    "            reduction_indices=1\n",
    "        )\n",
    "        \n",
    "        # Apply KL tolerance\n",
    "        self.kl_loss = tf.maximum(self.kl_loss, self.kl_tolerance * self.z_size)\n",
    "        self.kl_loss = tf.reduce_mean(self.kl_loss)\n",
    "        \n",
    "        # Total loss\n",
    "        self.loss = self.reconstruction_loss + self.kl_loss\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def _init_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _init_saver(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def close_sess(self):\n",
    "        self.sess.close()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.sess.run(self.z, feed_dict={self.x: x})\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.sess.run(self.y, feed_dict={self.z: z})\n",
    "    \n",
    "    def get_model_params(self):\n",
    "        # Get trainable variables\n",
    "        params = self.sess.run(tf.trainable_variables())\n",
    "        return params\n",
    "    \n",
    "    def set_model_params(self, params):\n",
    "        # Set trainable variables\n",
    "        var_list = tf.trainable_variables()\n",
    "        ops = []\n",
    "        for i, var in enumerate(var_list):\n",
    "            ops.append(tf.assign(var, params[i]))\n",
    "        self.sess.run(ops)\n",
    "    \n",
    "    def save_json(self, json_path):\n",
    "        # Save model parameters as JSON\n",
    "        params = self.get_model_params()\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_json(self, json_path):\n",
    "        # Load model parameters from JSON\n",
    "        with open(json_path, 'r') as f:\n",
    "            params = json.load(f)\n",
    "        self.set_model_params(params)\n",
    "    \n",
    "    def train(self, x):\n",
    "        # Train the VAE\n",
    "        _, loss, r_loss, kl_loss = self.sess.run(\n",
    "            [self.train_op, self.loss, self.reconstruction_loss, self.kl_loss],\n",
    "            feed_dict={self.x: x}\n",
    "        )\n",
    "        return loss, r_loss, kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83cab8",
   "metadata": {},
   "source": [
    "## 5. Implementing the MDN-RNN (Memory)\n",
    "\n",
    "The MDN-RNN takes the latent representation from the VAE and predicts future latent states based on the current state and the action. It uses a mixture density network output layer to model the uncertainty in the predictions.\n",
    "\n",
    "The architecture consists of:\n",
    "- LSTM layers: To capture temporal dependencies\n",
    "- Mixture Density Network: To model the distribution of the next latent state\n",
    "\n",
    "Let's implement the MDN-RNN for VizDoom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the MDN-RNN implementation for VizDoom\n",
    "# In the Docker container, this would be in doomrnn/rnn.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "class MDNRNN(object):\n",
    "    def __init__(self, z_size=64, action_size=2, hidden_units=256, \n",
    "                 n_mixtures=5, batch_size=100, learning_rate=0.001, grad_clip=1.0,\n",
    "                 is_training=True, reuse=False):\n",
    "        self.z_size = z_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_mixtures = n_mixtures\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = grad_clip\n",
    "        self.is_training = is_training\n",
    "        self.reuse = reuse\n",
    "        \n",
    "        with tf.variable_scope('mdn_rnn', reuse=self.reuse):\n",
    "            self._build_graph()\n",
    "            self._init_session()\n",
    "            self._init_saver()\n",
    "            \n",
    "    def _build_graph(self):\n",
    "        # Placeholders\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, None, self.z_size + self.action_size])\n",
    "        self.y = tf.placeholder(tf.float32, shape=[None, None, self.z_size])\n",
    "        self.seq_lengths = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.initial_state = None\n",
    "        \n",
    "        # LSTM layers\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units)\n",
    "        self.initial_state = lstm_cell.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "        \n",
    "        # RNN\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(\n",
    "            lstm_cell, \n",
    "            self.x,\n",
    "            initial_state=self.initial_state,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=self.seq_lengths\n",
    "        )\n",
    "        \n",
    "        # Reshape outputs for dense layers\n",
    "        outputs_flat = tf.reshape(outputs, [-1, self.hidden_units])\n",
    "        \n",
    "        # MDN outputs\n",
    "        # For each mixture, we need:\n",
    "        # - pi: the mixture weight\n",
    "        # - mu: the mean\n",
    "        # - sigma: the standard deviation\n",
    "        n_outputs = self.n_mixtures * (2 * self.z_size + 1)\n",
    "        \n",
    "        # Dense layer\n",
    "        mdn_outputs = tf.layers.dense(outputs_flat, n_outputs)\n",
    "        \n",
    "        # Split MDN outputs\n",
    "        mdn_out_pi, mdn_out_mu, mdn_out_sigma = self._split_mdn_outputs(mdn_outputs)\n",
    "        \n",
    "        # Apply softmax to pi\n",
    "        mdn_out_pi = tf.nn.softmax(mdn_out_pi, axis=-1)\n",
    "        \n",
    "        # Apply exp to sigma to ensure it's positive\n",
    "        mdn_out_sigma = tf.exp(mdn_out_sigma)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss = self._mdn_loss(mdn_out_pi, mdn_out_mu, mdn_out_sigma, self.y)\n",
    "        \n",
    "        # Optimizer with gradient clipping\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        grads, vars = zip(*optimizer.compute_gradients(self.loss))\n",
    "        grads, _ = tf.clip_by_global_norm(grads, self.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, vars))\n",
    "        \n",
    "    def _split_mdn_outputs(self, mdn_outputs):\n",
    "        # Split the MDN outputs into pi, mu, and sigma\n",
    "        mdn_output_shape = tf.shape(mdn_outputs)\n",
    "        batch_size = mdn_output_shape[0]\n",
    "        \n",
    "        # Split along the last dimension\n",
    "        mdn_out_pi = mdn_outputs[:, :self.n_mixtures]\n",
    "        mdn_out_mu = mdn_outputs[:, self.n_mixtures:self.n_mixtures*(1+self.z_size)]\n",
    "        mdn_out_sigma = mdn_outputs[:, self.n_mixtures*(1+self.z_size):]\n",
    "        \n",
    "        # Reshape mu and sigma\n",
    "        mdn_out_mu = tf.reshape(mdn_out_mu, [batch_size, self.n_mixtures, self.z_size])\n",
    "        mdn_out_sigma = tf.reshape(mdn_out_sigma, [batch_size, self.n_mixtures, self.z_size])\n",
    "        \n",
    "        return mdn_out_pi, mdn_out_mu, mdn_out_sigma\n",
    "    \n",
    "    def _mdn_loss(self, pi, mu, sigma, y):\n",
    "        # Calculate the MDN loss\n",
    "        y = tf.reshape(y, [-1, 1, self.z_size])\n",
    "        \n",
    "        # Calculate the negative log likelihood\n",
    "        dist = tf.contrib.distributions.Normal(loc=mu, scale=sigma)\n",
    "        log_prob = dist.log_prob(y)\n",
    "        log_prob = tf.reduce_sum(log_prob, axis=-1)\n",
    "        \n",
    "        # Weight by the mixture probabilities\n",
    "        weighted_log_prob = log_prob + tf.log(pi + 1e-8)\n",
    "        \n",
    "        # Use logsumexp for numerical stability\n",
    "        max_weighted_log_prob = tf.reduce_max(weighted_log_prob, axis=-1, keepdims=True)\n",
    "        log_likelihood = max_weighted_log_prob + tf.log(tf.reduce_sum(\n",
    "            tf.exp(weighted_log_prob - max_weighted_log_prob), axis=-1))\n",
    "        \n",
    "        # Negative log likelihood\n",
    "        loss = -tf.reduce_mean(log_likelihood)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _init_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _init_saver(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def close_sess(self):\n",
    "        self.sess.close()\n",
    "        \n",
    "    def get_model_params(self):\n",
    "        # Get trainable variables\n",
    "        params = self.sess.run(tf.trainable_variables())\n",
    "        return params\n",
    "    \n",
    "    def set_model_params(self, params):\n",
    "        # Set trainable variables\n",
    "        var_list = tf.trainable_variables()\n",
    "        ops = []\n",
    "        for i, var in enumerate(var_list):\n",
    "            ops.append(tf.assign(var, params[i]))\n",
    "        self.sess.run(ops)\n",
    "    \n",
    "    def save_json(self, json_path):\n",
    "        # Save model parameters as JSON\n",
    "        params = self.get_model_params()\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_json(self, json_path):\n",
    "        # Load model parameters from JSON\n",
    "        with open(json_path, 'r') as f:\n",
    "            params = json.load(f)\n",
    "        self.set_model_params(params)\n",
    "    \n",
    "    def train(self, x, y, seq_lengths):\n",
    "        # Train the MDN-RNN\n",
    "        _, loss = self.sess.run(\n",
    "            [self.train_op, self.loss],\n",
    "            feed_dict={self.x: x, self.y: y, self.seq_lengths: seq_lengths}\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, x, state=None):\n",
    "        # Predict the next latent state\n",
    "        # This is not implemented in this simplified version\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849655d",
   "metadata": {},
   "source": [
    "## 6. Implementing the Controller\n",
    "\n",
    "The Controller is a simple neural network that maps latent states to actions. It's trained using evolutionary strategies (specifically CMA-ES) to maximize the reward.\n",
    "\n",
    "The architecture is simple:\n",
    "- Input: Latent state from the VAE\n",
    "- Hidden layers: A few fully connected layers\n",
    "- Output: Action probabilities\n",
    "\n",
    "Let's implement the Controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Controller implementation for VizDoom\n",
    "# In the Docker container, this would be in doomrnn/controller.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "class Controller(object):\n",
    "    def __init__(self, z_size=64, action_size=2, hidden_units=40, batch_size=100, learning_rate=0.001):\n",
    "        self.z_size = z_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self._build_graph()\n",
    "        self._init_session()\n",
    "        self._init_saver()\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        # Placeholders\n",
    "        self.z = tf.placeholder(tf.float32, shape=[None, self.z_size])\n",
    "        \n",
    "        # Hidden layers\n",
    "        h = tf.layers.dense(self.z, self.hidden_units, activation=tf.nn.tanh)\n",
    "        h = tf.layers.dense(h, self.hidden_units, activation=tf.nn.tanh)\n",
    "        \n",
    "        # Output layer\n",
    "        self.action_logits = tf.layers.dense(h, self.action_size)\n",
    "        self.action_probs = tf.nn.softmax(self.action_logits)\n",
    "        \n",
    "    def _init_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _init_saver(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def close_sess(self):\n",
    "        self.sess.close()\n",
    "        \n",
    "    def get_action(self, z):\n",
    "        # Get the action probabilities\n",
    "        action_probs = self.sess.run(self.action_probs, feed_dict={self.z: z})\n",
    "        return action_probs\n",
    "    \n",
    "    def get_model_params(self):\n",
    "        # Get trainable variables\n",
    "        params = self.sess.run(tf.trainable_variables())\n",
    "        return params\n",
    "    \n",
    "    def set_model_params(self, params):\n",
    "        # Set trainable variables\n",
    "        var_list = tf.trainable_variables()\n",
    "        ops = []\n",
    "        for i, var in enumerate(var_list):\n",
    "            ops.append(tf.assign(var, params[i]))\n",
    "        self.sess.run(ops)\n",
    "    \n",
    "    def save_json(self, json_path):\n",
    "        # Save model parameters as JSON\n",
    "        params = self.get_model_params()\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    \n",
    "    def load_json(self, json_path):\n",
    "        # Load model parameters from JSON\n",
    "        with open(json_path, 'r') as f:\n",
    "            params = json.load(f)\n",
    "        self.set_model_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717ba5b",
   "metadata": {},
   "source": [
    "## 7. Data Collection and Preprocessing\n",
    "\n",
    "Before we can train our models, we need to collect data from the VizDoom environment. We'll use random actions to explore the environment and collect observations.\n",
    "\n",
    "For the VAE, we need images from the environment.\n",
    "For the MDN-RNN, we need sequences of latent states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd94b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection for VizDoom\n",
    "# In the Docker container, this would be executed directly\n",
    "\n",
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def collect_data(num_episodes=100, max_steps=1000, data_dir=\"data\"):\n",
    "    \"\"\"Collect data from VizDoom environment using random actions\"\"\"\n",
    "    # Create the environment\n",
    "    env = gym.make('ppaquette/DoomTakeCover-v0')\n",
    "    \n",
    "    # Create data directory\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Collect data\n",
    "    total_frames = 0\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment\n",
    "        observation = env.reset()\n",
    "        \n",
    "        # Downsample and normalize the observation\n",
    "        obs = process_frame(observation)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Take a random action\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # Step the environment\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Downsample and normalize the next observation\n",
    "            next_obs = process_frame(next_observation)\n",
    "            \n",
    "            # Save the observation and action\n",
    "            save_frame(obs, os.path.join(data_dir, f\"frame_{total_frames}.png\"))\n",
    "            \n",
    "            # Update counters\n",
    "            total_frames += 1\n",
    "            obs = next_obs\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print(f\"Episode {episode+1}/{num_episodes} completed, total frames: {total_frames}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"Data collection completed. Total frames: {total_frames}\")\n",
    "    \n",
    "def process_frame(frame, target_size=(64, 64)):\n",
    "    \"\"\"Preprocess a frame: resize and normalize\"\"\"\n",
    "    # Convert to PIL Image\n",
    "    img = Image.fromarray(frame)\n",
    "    \n",
    "    # Resize\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert back to numpy array and normalize\n",
    "    img_array = np.array(img) / 255.0\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def save_frame(frame, filepath):\n",
    "    \"\"\"Save a frame as an image file\"\"\"\n",
    "    # Convert to PIL Image\n",
    "    img = Image.fromarray((frame * 255).astype(np.uint8))\n",
    "    \n",
    "    # Save the image\n",
    "    img.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8438c",
   "metadata": {},
   "source": [
    "## 8. Training the VAE\n",
    "\n",
    "Now that we have collected data, we can train the VAE to compress the observations into a latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the VAE\n",
    "# In the Docker container, this would be in doomrnn/vae_train.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import glob\n",
    "import json\n",
    "from vae import ConvVAE\n",
    "\n",
    "def load_frames(data_dir=\"data\", batch_size=100):\n",
    "    \"\"\"Load frames from the data directory\"\"\"\n",
    "    # Get all frame files\n",
    "    frame_files = glob.glob(os.path.join(data_dir, \"frame_*.png\"))\n",
    "    \n",
    "    # Shuffle the files\n",
    "    np.random.shuffle(frame_files)\n",
    "    \n",
    "    # Load frames in batches\n",
    "    n_files = len(frame_files)\n",
    "    n_batches = n_files // batch_size\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        batch_files = frame_files[i*batch_size:(i+1)*batch_size]\n",
    "        batch_frames = []\n",
    "        \n",
    "        for file in batch_files:\n",
    "            # Load the image\n",
    "            img = Image.open(file)\n",
    "            img_array = np.array(img) / 255.0\n",
    "            batch_frames.append(img_array)\n",
    "            \n",
    "        yield np.array(batch_frames)\n",
    "\n",
    "def train_vae(data_dir=\"data\", model_dir=\"vae\", batch_size=100, num_epochs=10, z_size=64):\n",
    "    \"\"\"Train the VAE on the collected data\"\"\"\n",
    "    # Create model directory\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Create the VAE\n",
    "    vae = ConvVAE(z_size=z_size, batch_size=batch_size)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Epoch stats\n",
    "        epoch_loss = 0\n",
    "        epoch_r_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Load data batches\n",
    "        for batch in load_frames(data_dir, batch_size):\n",
    "            # Train on batch\n",
    "            loss, r_loss, kl_loss = vae.train(batch)\n",
    "            \n",
    "            # Update stats\n",
    "            epoch_loss += loss\n",
    "            epoch_r_loss += r_loss\n",
    "            epoch_kl_loss += kl_loss\n",
    "            n_batches += 1\n",
    "            \n",
    "        # Print epoch stats\n",
    "        epoch_loss /= n_batches\n",
    "        epoch_r_loss /= n_batches\n",
    "        epoch_kl_loss /= n_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Reconstruction Loss: {epoch_r_loss:.4f}, KL Loss: {epoch_kl_loss:.4f}\")\n",
    "        \n",
    "        # Save model\n",
    "        vae.save_json(os.path.join(model_dir, \"vae.json\"))\n",
    "        \n",
    "    print(\"VAE training completed.\")\n",
    "    vae.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57e4a2",
   "metadata": {},
   "source": [
    "## 9. Training the MDN-RNN\n",
    "\n",
    "After training the VAE, we can use it to encode observations into latent states and train the MDN-RNN to predict future latent states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38088d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the MDN-RNN\n",
    "# In the Docker container, this would be in doomrnn/rnn_train.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "from PIL import Image\n",
    "from vae import ConvVAE\n",
    "from rnn import MDNRNN\n",
    "\n",
    "def generate_rnn_data(vae, num_episodes=100, max_steps=1000, data_dir=\"rnn_data\"):\n",
    "    \"\"\"Generate data for RNN training using the trained VAE\"\"\"\n",
    "    # Create the environment\n",
    "    env = gym.make('ppaquette/DoomTakeCover-v0')\n",
    "    \n",
    "    # Create data directory\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Collect data\n",
    "    z_series = []\n",
    "    action_series = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment\n",
    "        observation = env.reset()\n",
    "        \n",
    "        # Preprocess the observation\n",
    "        obs = process_frame(observation)\n",
    "        \n",
    "        # Encode the observation\n",
    "        z = vae.encode(np.array([obs]))[0]\n",
    "        \n",
    "        # Initialize episode data\n",
    "        episode_z = [z]\n",
    "        episode_actions = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Take a random action\n",
    "            action_idx = np.random.randint(0, env.action_space.n)\n",
    "            action = np.zeros(env.action_space.n)\n",
    "            action[action_idx] = 1\n",
    "            \n",
    "            # Step the environment\n",
    "            next_observation, reward, done, info = env.step(action_idx)\n",
    "            \n",
    "            # Preprocess the next observation\n",
    "            next_obs = process_frame(next_observation)\n",
    "            \n",
    "            # Encode the next observation\n",
    "            next_z = vae.encode(np.array([next_obs]))[0]\n",
    "            \n",
    "            # Store the action and next latent state\n",
    "            episode_actions.append(action)\n",
    "            episode_z.append(next_z)\n",
    "            \n",
    "            # Update observation\n",
    "            obs = next_obs\n",
    "            z = next_z\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Store the episode data\n",
    "        z_series.append(np.array(episode_z))\n",
    "        action_series.append(np.array(episode_actions))\n",
    "        \n",
    "        print(f\"Episode {episode+1}/{num_episodes} completed, steps: {len(episode_z)}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Save the data\n",
    "    np.save(os.path.join(data_dir, \"z_series.npy\"), z_series)\n",
    "    np.save(os.path.join(data_dir, \"action_series.npy\"), action_series)\n",
    "    \n",
    "    print(f\"RNN data generation completed. Episodes: {len(z_series)}\")\n",
    "    \n",
    "    return z_series, action_series\n",
    "\n",
    "def train_rnn(z_series, action_series, model_dir=\"rnn\", batch_size=100, num_epochs=10, z_size=64, action_size=8):\n",
    "    \"\"\"Train the MDN-RNN on the generated data\"\"\"\n",
    "    # Create model directory\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Create the RNN\n",
    "    rnn = MDNRNN(z_size=z_size, action_size=action_size, batch_size=batch_size)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Epoch stats\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Shuffle the episodes\n",
    "        indices = np.random.permutation(len(z_series))\n",
    "        \n",
    "        # Train on each episode\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            # Get batch indices\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            actual_batch_size = len(batch_indices)\n",
    "            \n",
    "            # Skip if batch is too small\n",
    "            if actual_batch_size < batch_size:\n",
    "                continue\n",
    "                \n",
    "            # Get batch episodes\n",
    "            batch_z = [z_series[idx] for idx in batch_indices]\n",
    "            batch_actions = [action_series[idx] for idx in batch_indices]\n",
    "            \n",
    "            # Create input sequences (z and action) and output sequences (next z)\n",
    "            max_seq_len = max(len(z) for z in batch_z)\n",
    "            seq_lengths = [len(z)-1 for z in batch_z]  # -1 because we need the next z\n",
    "            \n",
    "            # Skip if sequences are too short\n",
    "            if max(seq_lengths) <= 0:\n",
    "                continue\n",
    "                \n",
    "            # Create padded sequences\n",
    "            x_seq = np.zeros((batch_size, max_seq_len-1, z_size + action_size))\n",
    "            y_seq = np.zeros((batch_size, max_seq_len-1, z_size))\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                z = batch_z[b]\n",
    "                actions = batch_actions[b]\n",
    "                seq_len = min(len(z)-1, max_seq_len-1)\n",
    "                \n",
    "                for t in range(seq_len):\n",
    "                    # Input: concatenate z and action\n",
    "                    x_seq[b, t, :z_size] = z[t]\n",
    "                    x_seq[b, t, z_size:] = actions[t]\n",
    "                    \n",
    "                    # Output: next z\n",
    "                    y_seq[b, t] = z[t+1]\n",
    "            \n",
    "            # Train on batch\n",
    "            loss = rnn.train(x_seq, y_seq, seq_lengths)\n",
    "            \n",
    "            # Update stats\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "            \n",
    "        # Print epoch stats\n",
    "        epoch_loss /= max(1, n_batches)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Save model\n",
    "        rnn.save_json(os.path.join(model_dir, \"rnn.json\"))\n",
    "        \n",
    "    print(\"RNN training completed.\")\n",
    "    rnn.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d9096",
   "metadata": {},
   "source": [
    "## 10. Training the Controller with CMA-ES\n",
    "\n",
    "Finally, we can train the Controller using evolutionary strategies (CMA-ES) to maximize the reward in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Controller with CMA-ES\n",
    "# In the Docker container, this would be in doomrnn/train.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "import cma\n",
    "from PIL import Image\n",
    "from vae import ConvVAE\n",
    "from rnn import MDNRNN\n",
    "from controller import Controller\n",
    "\n",
    "def process_frame(frame, target_size=(64, 64)):\n",
    "    \"\"\"Preprocess a frame: resize and normalize\"\"\"\n",
    "    # Convert to PIL Image\n",
    "    img = Image.fromarray(frame)\n",
    "    \n",
    "    # Resize\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert back to numpy array and normalize\n",
    "    img_array = np.array(img) / 255.0\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def evaluate_controller(vae, controller, n_trials=3, max_steps=1000):\n",
    "    \"\"\"Evaluate the controller in the environment\"\"\"\n",
    "    # Create the environment\n",
    "    env = gym.make('ppaquette/DoomTakeCover-v0')\n",
    "    \n",
    "    # Run multiple trials and average the rewards\n",
    "    rewards = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Reset the environment\n",
    "        observation = env.reset()\n",
    "        \n",
    "        # Preprocess the observation\n",
    "        obs = process_frame(observation)\n",
    "        \n",
    "        # Encode the observation\n",
    "        z = vae.encode(np.array([obs]))\n",
    "        \n",
    "        # Initialize episode reward\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Get action probabilities\n",
    "            action_probs = controller.get_action(z)[0]\n",
    "            \n",
    "            # Sample action\n",
    "            action_idx = np.random.choice(len(action_probs), p=action_probs)\n",
    "            \n",
    "            # Step the environment\n",
    "            next_observation, reward, done, info = env.step(action_idx)\n",
    "            \n",
    "            # Update total reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            # Preprocess the next observation\n",
    "            next_obs = process_frame(next_observation)\n",
    "            \n",
    "            # Encode the next observation\n",
    "            z = vae.encode(np.array([next_obs]))\n",
    "            \n",
    "        rewards.append(total_reward)\n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "    # Return the average reward\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def train_controller_cma(vae_path=\"vae/vae.json\", model_dir=\"controller\", z_size=64, action_size=8, \n",
    "                         hidden_units=40, sigma_init=0.5, popsize=64, num_generations=100):\n",
    "    \"\"\"Train the controller using CMA-ES\"\"\"\n",
    "    # Create model directory\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the VAE\n",
    "    vae = ConvVAE(z_size=z_size, batch_size=1, is_training=False)\n",
    "    vae.load_json(vae_path)\n",
    "    \n",
    "    # Create the controller\n",
    "    controller = Controller(z_size=z_size, action_size=action_size, hidden_units=hidden_units)\n",
    "    \n",
    "    # Get the initial parameters\n",
    "    init_params = controller.get_model_params()\n",
    "    param_count = sum(p.size for p in init_params)\n",
    "    \n",
    "    # Flatten the parameters\n",
    "    init_params_flat = np.concatenate([p.flatten() for p in init_params])\n",
    "    \n",
    "    # Initialize CMA-ES\n",
    "    es = cma.CMAEvolutionStrategy(init_params_flat, sigma_init, \n",
    "                                   {'popsize': popsize, 'maxiter': num_generations})\n",
    "    \n",
    "    # Training loop\n",
    "    best_reward = -np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    for generation in range(num_generations):\n",
    "        # Sample solutions\n",
    "        solutions = es.ask()\n",
    "        \n",
    "        # Evaluate solutions\n",
    "        rewards = []\n",
    "        \n",
    "        for i, solution in enumerate(solutions):\n",
    "            # Reshape the solution\n",
    "            solution_params = []\n",
    "            start_idx = 0\n",
    "            \n",
    "            for p in init_params:\n",
    "                param_size = p.size\n",
    "                solution_params.append(solution[start_idx:start_idx+param_size].reshape(p.shape))\n",
    "                start_idx += param_size\n",
    "                \n",
    "            # Set the controller parameters\n",
    "            controller.set_model_params(solution_params)\n",
    "            \n",
    "            # Evaluate the controller\n",
    "            reward = evaluate_controller(vae, controller)\n",
    "            rewards.append(-reward)  # CMA-ES minimizes, so we negate the reward\n",
    "            \n",
    "            print(f\"Generation {generation+1}/{num_generations}, Solution {i+1}/{popsize}, Reward: {-rewards[-1]:.2f}\")\n",
    "            \n",
    "        # Update CMA-ES\n",
    "        es.tell(solutions, rewards)\n",
    "        \n",
    "        # Check for new best\n",
    "        best_idx = np.argmin(rewards)\n",
    "        if -rewards[best_idx] > best_reward:\n",
    "            best_reward = -rewards[best_idx]\n",
    "            best_solution = solutions[best_idx]\n",
    "            \n",
    "            # Reshape the best solution\n",
    "            best_params = []\n",
    "            start_idx = 0\n",
    "            \n",
    "            for p in init_params:\n",
    "                param_size = p.size\n",
    "                best_params.append(best_solution[start_idx:start_idx+param_size].reshape(p.shape))\n",
    "                start_idx += param_size\n",
    "                \n",
    "            # Save the best parameters\n",
    "            controller.set_model_params(best_params)\n",
    "            controller.save_json(os.path.join(model_dir, \"controller.json\"))\n",
    "            \n",
    "            print(f\"New best reward: {best_reward:.2f}\")\n",
    "            \n",
    "        # Print generation stats\n",
    "        print(f\"Generation {generation+1}/{num_generations}, Mean Reward: {-np.mean(rewards):.2f}, Best Reward: {best_reward:.2f}\")\n",
    "        \n",
    "        # Save generation stats\n",
    "        with open(os.path.join(model_dir, f\"gen_{generation+1}.json\"), 'w') as f:\n",
    "            json.dump({\n",
    "                'generation': generation+1,\n",
    "                'mean_reward': -np.mean(rewards),\n",
    "                'best_reward': best_reward\n",
    "            }, f)\n",
    "            \n",
    "    print(\"Controller training completed.\")\n",
    "    vae.close_sess()\n",
    "    controller.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce90d52",
   "metadata": {},
   "source": [
    "## 11. Running the Full Training Pipeline\n",
    "\n",
    "Now that we have implemented all components, let's run the full training pipeline to train our World Models for VizDoom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training pipeline\n",
    "# In the Docker container, this would be executed directly\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "data_dir = \"data\"\n",
    "vae_dir = \"vae\"\n",
    "rnn_dir = \"rnn\"\n",
    "controller_dir = \"controller\"\n",
    "z_size = 64\n",
    "action_size = 8\n",
    "hidden_units = 40\n",
    "batch_size = 100\n",
    "num_vae_epochs = 10\n",
    "num_rnn_epochs = 10\n",
    "num_controller_generations = 100\n",
    "\n",
    "# 1. Collect data for VAE training\n",
    "print(\"Step 1: Collecting data for VAE training...\")\n",
    "collect_data(num_episodes=100, max_steps=1000, data_dir=data_dir)\n",
    "\n",
    "# 2. Train VAE\n",
    "print(\"\\nStep 2: Training VAE...\")\n",
    "train_vae(data_dir=data_dir, model_dir=vae_dir, batch_size=batch_size, num_epochs=num_vae_epochs, z_size=z_size)\n",
    "\n",
    "# 3. Generate data for RNN training\n",
    "print(\"\\nStep 3: Generating data for RNN training...\")\n",
    "vae = ConvVAE(z_size=z_size, batch_size=1, is_training=False)\n",
    "vae.load_json(os.path.join(vae_dir, \"vae.json\"))\n",
    "z_series, action_series = generate_rnn_data(vae, num_episodes=100, max_steps=1000, data_dir=\"rnn_data\")\n",
    "vae.close_sess()\n",
    "\n",
    "# 4. Train RNN\n",
    "print(\"\\nStep 4: Training RNN...\")\n",
    "train_rnn(z_series, action_series, model_dir=rnn_dir, batch_size=batch_size, num_epochs=num_rnn_epochs, \n",
    "          z_size=z_size, action_size=action_size)\n",
    "\n",
    "# 5. Train Controller with CMA-ES\n",
    "print(\"\\nStep 5: Training Controller with CMA-ES...\")\n",
    "train_controller_cma(vae_path=os.path.join(vae_dir, \"vae.json\"), model_dir=controller_dir, z_size=z_size, \n",
    "                     action_size=action_size, hidden_units=hidden_units, num_generations=num_controller_generations)\n",
    "\n",
    "print(\"\\nTraining pipeline completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83afe2c5",
   "metadata": {},
   "source": [
    "## 12. Visualizing Training Progress\n",
    "\n",
    "Let's visualize the training progress of the CMA-ES algorithm for the Controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807717f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing training progress\n",
    "# In the Docker container, this would be executed directly\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_training_progress(controller_dir=\"controller\", num_generations=100):\n",
    "    \"\"\"Plot the training progress of the Controller\"\"\"\n",
    "    # Collect generation stats\n",
    "    generations = []\n",
    "    mean_rewards = []\n",
    "    best_rewards = []\n",
    "    \n",
    "    for gen in range(1, num_generations+1):\n",
    "        gen_file = os.path.join(controller_dir, f\"gen_{gen}.json\")\n",
    "        \n",
    "        if os.path.exists(gen_file):\n",
    "            with open(gen_file, 'r') as f:\n",
    "                stats = json.load(f)\n",
    "                \n",
    "            generations.append(stats['generation'])\n",
    "            mean_rewards.append(stats['mean_reward'])\n",
    "            best_rewards.append(stats['best_reward'])\n",
    "    \n",
    "    # Plot the progress\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(generations, mean_rewards, 'b-', label='Mean Reward')\n",
    "    plt.plot(generations, best_rewards, 'r-', label='Best Reward')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('CMA-ES Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(controller_dir, 'training_progress.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training progress\n",
    "plot_training_progress(controller_dir=controller_dir, num_generations=num_controller_generations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b15675",
   "metadata": {},
   "source": [
    "## 13. Testing the Trained Model\n",
    "\n",
    "Finally, let's test our trained World Models in the VizDoom environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb51fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the trained model\n",
    "# In the Docker container, this would be executed directly\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "def test_model(vae_path=\"vae/vae.json\", controller_path=\"controller/controller.json\", \n",
    "               z_size=64, action_size=8, hidden_units=40, num_episodes=5, max_steps=1000, \n",
    "               render=True):\n",
    "    \"\"\"Test the trained model in the environment\"\"\"\n",
    "    # Load the VAE\n",
    "    vae = ConvVAE(z_size=z_size, batch_size=1, is_training=False)\n",
    "    vae.load_json(vae_path)\n",
    "    \n",
    "    # Load the Controller\n",
    "    controller = Controller(z_size=z_size, action_size=action_size, hidden_units=hidden_units)\n",
    "    controller.load_json(controller_path)\n",
    "    \n",
    "    # Create the environment\n",
    "    env = gym.make('ppaquette/DoomTakeCover-v0')\n",
    "    \n",
    "    # Run episodes\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment\n",
    "        observation = env.reset()\n",
    "        \n",
    "        # Initialize episode data\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Preprocess the observation\n",
    "            obs = process_frame(observation)\n",
    "            \n",
    "            # Save the frame if rendering is enabled\n",
    "            if render:\n",
    "                frames.append(observation)\n",
    "                \n",
    "            # Encode the observation\n",
    "            z = vae.encode(np.array([obs]))\n",
    "            \n",
    "            # Get action probabilities\n",
    "            action_probs = controller.get_action(z)[0]\n",
    "            \n",
    "            # Sample action\n",
    "            action_idx = np.random.choice(len(action_probs), p=action_probs)\n",
    "            \n",
    "            # Step the environment\n",
    "            observation, reward, done, info = env.step(action_idx)\n",
    "            \n",
    "            # Update total reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Reward: {total_reward:.2f}\")\n",
    "        \n",
    "        # Display a few frames if rendering is enabled\n",
    "        if render and len(frames) > 0:\n",
    "            # Display a subset of frames\n",
    "            n_frames = min(5, len(frames))\n",
    "            fig, axes = plt.subplots(1, n_frames, figsize=(20, 4))\n",
    "            \n",
    "            for i, frame_idx in enumerate(np.linspace(0, len(frames)-1, n_frames).astype(int)):\n",
    "                axes[i].imshow(frames[frame_idx])\n",
    "                axes[i].axis('off')\n",
    "                axes[i].set_title(f\"Frame {frame_idx}\")\n",
    "                \n",
    "            plt.suptitle(f\"Episode {episode+1}, Reward: {total_reward:.2f}\")\n",
    "            plt.show()\n",
    "    \n",
    "    env.close()\n",
    "    vae.close_sess()\n",
    "    controller.close_sess()\n",
    "    \n",
    "    # Print overall stats\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    print(f\"\\nTest completed, Mean Reward: {mean_reward:.2f}  {std_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "# Test the trained model\n",
    "test_model(vae_path=os.path.join(vae_dir, \"vae.json\"), \n",
    "           controller_path=os.path.join(controller_dir, \"controller.json\"), \n",
    "           z_size=z_size, action_size=action_size, hidden_units=hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e63da",
   "metadata": {},
   "source": [
    "## 14. Conclusion and Next Steps\n",
    "\n",
    "We have successfully implemented and trained a World Models architecture for the VizDoom environment. The key components are:\n",
    "\n",
    "1. **VAE (Vision)**: Compresses high-dimensional visual input into a low-dimensional latent representation.\n",
    "2. **MDN-RNN (Memory)**: Predicts future latent states based on current states and actions.\n",
    "3. **Controller**: Maps latent states to actions using a simple neural network trained with CMA-ES.\n",
    "\n",
    "This implementation follows the architecture described in the World Models paper, but with some simplifications for clarity.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "There are several ways to extend and improve this implementation:\n",
    "\n",
    "1. **Temperature Parameter**: Add a temperature parameter to the MDN-RNN to control the stochasticity of the predictions.\n",
    "2. **Dream Training**: Train the controller purely in the dream environment (using only the RNN for state predictions).\n",
    "3. **Longer Training**: Train the models for more epochs/generations to improve performance.\n",
    "4. **Hyperparameter Tuning**: Experiment with different hyperparameters for each component.\n",
    "5. **Different Environments**: Apply the same architecture to other VizDoom scenarios or different environments.\n",
    "\n",
    "### References\n",
    "\n",
    "- Ha, D., & Schmidhuber, J. (2018). World models. arXiv preprint arXiv:1803.10122.\n",
    "- [World Models Website](https://worldmodels.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fcf99e",
   "metadata": {},
   "source": [
    "# Copying Trained Models to the tf_models Directory\n",
    "\n",
    "Now that we have the VAE, RNN, and initial_z models trained, we need to copy these files to the `tf_models` subdirectory in the `doomrnn` folder. This will allow us to use these trained models for the CMA-ES evolutionary training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "os.makedirs('doomrnn/tf_models', exist_ok=True)\n",
    "\n",
    "# Copy the model files\n",
    "source_files = [\n",
    "    'vae/vae.json',\n",
    "    'initial_z/initial_z.json',\n",
    "    'rnn/rnn.json'\n",
    "]\n",
    "\n",
    "for source_file in source_files:\n",
    "    source_path = os.path.join('doomrnn', source_file)\n",
    "    target_path = os.path.join('doomrnn/tf_models', os.path.basename(source_file))\n",
    "    \n",
    "    if os.path.exists(source_path):\n",
    "        shutil.copy2(source_path, target_path)\n",
    "        print(f\"Copied {source_path} to {target_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: Source file {source_path} does not exist\")\n",
    "\n",
    "# Verify the files were copied\n",
    "print(\"\\nFiles in tf_models directory:\")\n",
    "for file in os.listdir('doomrnn/tf_models'):\n",
    "    if file.endswith('.json'):\n",
    "        print(f\" - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db2f51",
   "metadata": {},
   "source": [
    "# Updating Git Repository\n",
    "\n",
    "Now let's update our git repository with the new model files. We'll add the model files to the repository and commit the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6393e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using os.system to run git commands\n",
    "import os\n",
    "\n",
    "# Check git status\n",
    "print(\"Current git status:\")\n",
    "os.system(\"git status\")\n",
    "\n",
    "# Add the model files to git\n",
    "print(\"\\nAdding model files to git:\")\n",
    "os.system(\"git add doomrnn/tf_models/*.json\")\n",
    "\n",
    "# Commit the changes\n",
    "print(\"\\nCommitting changes:\")\n",
    "os.system('git commit -m \"Add trained VAE, RNN, and initial_z models for doom\"')\n",
    "\n",
    "# Show git status after commit\n",
    "print(\"\\nGit status after commit:\")\n",
    "os.system(\"git status\")\n",
    "\n",
    "# Optional: Push to your fork\n",
    "# Note: Uncomment the line below if you want to push to your fork\n",
    "# os.system(\"git push origin master\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d607f9",
   "metadata": {},
   "source": [
    "# CMA-ES Training Process\n",
    "\n",
    "After copying the model files and updating the git repository, we need to run the CMA-ES training process on a 64-core CPU instance. Here are the steps:\n",
    "\n",
    "1. **Start a 64-core CPU instance** (if you haven't already)\n",
    "2. **Log into the machine**\n",
    "3. **Navigate to the doomrnn directory**\n",
    "4. **Run the training script**: `python train.py`\n",
    "\n",
    "The training process will continue until you stop it with Ctrl+C. It's recommended to run for about 200 generations (4-5 hours), which should be sufficient to get good results.\n",
    "\n",
    "While training is running, you can monitor the progress using the `plot_training_progress.ipynb` notebook, which loads and visualizes the log files being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a551e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When on the 64-core CPU instance, run this command in the terminal:\n",
    "# cd doomrnn\n",
    "# python train.py\n",
    "\n",
    "# For reference, here's what the command would look like if executed from Python:\n",
    "import os\n",
    "\n",
    "# Don't run this cell directly unless you're on the 64-core CPU instance\n",
    "# This is just for reference\n",
    "def run_cma_es_training():\n",
    "    os.chdir('doomrnn')\n",
    "    os.system('python train.py')\n",
    "    \n",
    "# Note: The training will run until you manually stop it with Ctrl+C\n",
    "# It's recommended to run for ~200 generations (4-5 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d9832",
   "metadata": {},
   "source": [
    "# Monitoring Training Progress\n",
    "\n",
    "While the CMA-ES training is running, you can monitor its progress using the `plot_training_progress.ipynb` notebook. This notebook will load the log files being generated and create visualizations of the training progress.\n",
    "\n",
    "Let's prepare a cell that you can use to open this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To open the training progress notebook, run:\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "notebook_path = os.path.join('doomrnn', 'plot_training_progress.ipynb')\n",
    "\n",
    "if os.path.exists(notebook_path):\n",
    "    print(f\"Opening notebook at: {notebook_path}\")\n",
    "    if sys.platform == 'win32':\n",
    "        os.startfile(notebook_path)  # Windows-specific\n",
    "    elif sys.platform == 'darwin':  # macOS\n",
    "        subprocess.run(['open', notebook_path])\n",
    "    else:  # Linux\n",
    "        subprocess.run(['xdg-open', notebook_path])\n",
    "else:\n",
    "    print(f\"Warning: Notebook not found at {notebook_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298dc0eb",
   "metadata": {},
   "source": [
    "# Saving Training Results to Git\n",
    "\n",
    "After you've completed the training (recommended to run for about 200 generations or 4-5 hours), you'll need to add the log files to your git repository.\n",
    "\n",
    "These log files are stored in the `doomrnn/log` directory with `.json` extensions. They contain the training history and the best models found during training.\n",
    "\n",
    "Here's how to add them to your git repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using os.system to run git commands for saving log files\n",
    "import os\n",
    "\n",
    "# After training is complete, run these commands:\n",
    "\n",
    "# Check git status\n",
    "print(\"Current git status:\")\n",
    "os.system(\"git status\")\n",
    "\n",
    "# Add the log files to git\n",
    "print(\"\\nAdding log files to git:\")\n",
    "os.system(\"git add doomrnn/log/*.json\")\n",
    "\n",
    "# Commit the changes\n",
    "print(\"\\nCommitting changes:\")\n",
    "os.system('git commit -m \"Add CMA-ES training logs for doom\"')\n",
    "\n",
    "# Show git status after commit\n",
    "print(\"\\nGit status after commit:\")\n",
    "os.system(\"git status\")\n",
    "\n",
    "# Optional: Push to your fork\n",
    "# Note: Uncomment the line below if you want to push to your fork\n",
    "# os.system(\"git push origin master\")\n",
    "\n",
    "# Shutdown the instance after you're done\n",
    "print(\"\\nRemember to shutdown the instance after completing these steps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2f3fe",
   "metadata": {},
   "source": [
    "# Summary of Steps\n",
    "\n",
    "Here's a summary of the steps we've covered:\n",
    "\n",
    "1. **Copy trained models**:\n",
    "   - Copy VAE, RNN, and initial_z models to the tf_models directory\n",
    "   - Update git repository with these files\n",
    "\n",
    "2. **Run CMA-ES training**:\n",
    "   - Start a 64-core CPU instance\n",
    "   - Navigate to doomrnn directory\n",
    "   - Run `python train.py`\n",
    "   - Monitor progress with the plotting notebook\n",
    "   - Stop after ~200 generations (4-5 hours)\n",
    "\n",
    "3. **Save training results**:\n",
    "   - Add log files to git repository\n",
    "   - Commit changes\n",
    "   - Push to your fork (optional)\n",
    "   - Shutdown the instance\n",
    "\n",
    "These steps complete the training pipeline for the World Models approach in the VizDoom environment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
